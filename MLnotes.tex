\documentclass{report}
\usepackage[english]{babel}
\usepackage[letterpaper,top=3cm,bottom=3cm,left=3cm,right=3cm,marginparwidth=1.65cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{proposition}{Proposition}[chapter]

\begin{document}
\chapter*{Introduction}
These brief informal notes aim to concisely—but not exhaustively—illustrate some mathematical aspects of machine learning. The emphasis is on general conceptualization and the connections between this discipline and other mathematical fields, such as statistical inference, computational statistics, statistical learning, statistical decision theory, optimization theory, information theory, and others.

In broad terms, machine learning is a specialized subfield of artificial intelligence that draws on various techniques and knowledge from multiple mathematical domains to enable computers to solve problems that are intractable using traditional algorithmic methods. Unfortunately, these disciplines often use different terminologies, notations, and modes of reasoning, which can make it challenging for scientists who are not specialists in these areas to develop a unified understanding of the underlying principles.

These notes seek to bridge this gap by providing readers with a unified and synthetic overview of the general prerequisites and foundational concepts. However, they are not intended to replace authoritative papers and textbooks, where more thorough explanations and detailed examples can be found. Additionally, these notes do not cover the practical implementation of mathematical techniques using specific programming languages, frameworks, or libraries.

\chapter{Mathematical principles of machine learning}
In a broad sense, statistical inference and statistical machine learning share the common goal of understanding (inferring or learning) a certain phenomenon or process characterized by uncertainty, noisiness, and randomness. Typically, the unique information about the phenomenon is represented by a limited amount of empirical sample \textbf{data} generated by the mechanism underlying the phenomenon. This generation process is inherently random, producing data that can be viewed as realizations of appropriate \textbf{random variables}, distributed according to a specific probability distribution function (PDF) that is often unknown.

Ultimately, the objective is to reverse-engineer the process and infer general properties that apply to both observed and unobserved data. Thus, the most general problem in statistical inference and machine learning is to estimate, from observed data, the PDF that generates all data - both observed and unobserved - related to the phenomenon. Let us now provide a more precise definition of these concepts:

\begin{definition}
Let $X_1,\dots,X_N$ be a sample of random variables whose values represent observed \textbf{data}. The \textbf{statistical inference} (or \textbf{statistical machine learning}) problem aims to infer (or learn) the distribution that generated the data; that is, the probability distribution function (PDF) $f$ that describes both the observed data and potentially the not-yet-observed data:
\begin{equation}
X_1,\dots,X_N \sim f.
\end{equation}
\end{definition}

The last definition is completely general, and the distribution $f$ can be any distribution relevant to the problem under investigation; it may be the distribution of a single variable $X$ (denoted $f_X$), or a joint, marginal, or conditional distribution. In these notes, we will explore which distribution is most appropriate depending on the problem at hand. It is important to emphasize that Definition 1.1 only describes the general nature of the mathematical problem in machine learning, but does not specify how it is mathematically formulated or approximately solved.

Usually, the distribution $f$ is inferred (learned) by searching within a fixed set of probability density functions, which is called a \textbf{statistical model}. The inference problem is called \textbf{parametric} if any distribution in the statistical model ($f_\theta$) can be parameterized by quantities $\theta \in \Theta \subseteq \mathbb{R}^d$, which are referred to as \textbf{parameters} or \textbf{weights}; otherwise, it is called \textbf{non-parametric}. The process of performing inference from data, implemented in an algorithmic manner in computational contexts, is also known as learning (or fitting or training) in the machine learning literature.

\section{Statistical foundations}
In what follows, we focus primarily on \textbf{parametric} problems and assume that the random variables are \textbf{independent and identically distributed} (i.i.d.). We will adopt a \textbf{frequentist} approach, in which the parameters $\theta$ are fixed but unknown quantities.

The general problem outlined in Definition 1.1 can be formulated in a principled manner by employing concepts from \textbf{statistical decision theory}:

\begin{definition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$. Consider the statistical model $\left\{f_\theta :\theta\in\Theta\subseteq\mathbb{R}^d\right\}$. The \textbf{risk} (or \textbf{error}) of the estimator distribution $f_\theta$ with respect to the real distribution $f$ is defined as
\begin{equation}
R_L(f,f_\theta) := E_X[L(f(X),f_\theta(X))] = \int_{X(\Omega)} L(f(x),f_\theta(x))f(x)dx,
\end{equation}
where $L : [0,1]^2 \to \mathbb{R}$ is the \textbf{loss} function, and its expression $L(f(X), f_\theta(X))$ is a random variable.
\end{definition}

The risk (1.2) quantifies the error incurred when approximating the true distribution $f$ with the model distribution $f_\theta$. The optimal value of $f_\theta$ can then be determined by minimizing this risk.

By selecting a loss function with a logarithmic form, one can define the so-called Kullback-Leibler divergence:

\begin{definition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$. Consider the statistical model $\left\{f_\theta :\theta\in\Theta\subseteq\mathbb{R}^d\right\}$. The \textbf{Kullback–Leibler (KL) divergence} of the estimator distribution $f_\theta$ with respect to the real distribution $f$ is defined as the risk with a logarithmic loss function:
\begin{equation}
D_{KL}(f||f_\theta) := E_X\left [\ln \frac{f(X)}{f_\theta(X)} \right] = \int_{X(\Omega)} \ln \left ( \frac{f(x)}{f_\theta(x)} \right )f(x)dx.
\end{equation}
\end{definition}

The quantity (1.3) is one of the most important concepts in statistical inference and machine learning because it provides a fundamental way to measure how one probability distribution diverges from another one. This measure quantifies the expected information lost when using an approximate distribution instead of the true distribution and is widely used to assess differences between probability distributions. The KL divergence behaves like a distance in the sense that it satisfies the following properties:

\begin{equation}
 D_{KL}(f||g) \geq 0\,\,\,\,\,\,\,\,\,\, D_{KL}(f||g) = 0 \iff f = g.
\end{equation}
However, in general, the triangle inequality is not satisfied, so the KL divergence is not a metric in the formal sense of metric spaces.

Unfortunately, the risk (1.2) cannot be calculated if $f$ is unknown. An alternative measure of risk that utilizes the available data is the empirical risk, defined as follows:

\begin{definition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$, realized by the data set $D = (x_1\dots,x_N)$. Consider the statistical model $\left\{f_\theta : \theta \in \Theta \subseteq \mathbb{R}^d\right\}$. The \textbf{empirical risk (ER)} (also called \textbf{empirical error} or \textbf{training error}) of the estimator distribution $f_\theta$ with respect to the true distribution $f$ is defined as
\begin{equation}
\hat{R}_L(f,f_\theta)_N(D) := \frac{1}{N}\sum_{k=1}^{N}L(f(x_k),f_\theta(x_k)),
\end{equation}
where $L$ is the \textbf{loss} function.
\end{definition}

The empirical risk (1.5) encodes information about the observed data set and is clearly a function solely of the parameters $\theta$ because the data set is fixed. Moreover, it serves as a "good" approximation for (1.2) and, given a suitable loss function, can be minimized with respect to the parameters $\theta$ to find the best estimator distribution $f_\theta$. This procedure, known as \textbf{empirical risk minimization}, is a principled and general method for obtaining what are called \textbf{estimators} in statistical inference. Let us now define this concept:

\begin{definition}
Let $X_1,\dots,X_N$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$. Consider the model $\left\{f_\theta : \theta \in \Theta \subseteq \mathbb{R}^d\right \}$. A \textbf{statistic} is a random variable $H_N = h(X_1,\dots,X_N)$ such that $h : (\mathbb{R}^n)^N \to \mathbb{R}^d$ is a measurable function. If the image of $h$ is (a subset of) $\Theta$, then $H_N$ is named point \textbf{estimator} for $\theta \in \Theta$. In this latter case $H_N$ is indicated as $\hat{\theta}_N$.
\end{definition}

We have introduced a fundamental concept in machine learning: estimators. Estimators are random quantities that depend on the data used to train the model. Specifically, when the data are realized as a set of values $x_1,\dots,x_N$, the estimator $\hat{\theta}_N$ takes the value $\hat{\theta}_N(x_1,\dots,x_N) \in \Theta$. These estimators are the central quantities that are estimated (or learned) from data through the statistical machine learning algorithms that drive the inference (learning) process, as mentioned in Definition 1.1. Therefore, statistical machine learning algorithms perform statistical estimation to accomplish their tasks. It is important to emphasize that an estimator $\hat{\theta}_N$ is a random variable depending on data. Therefore, for
every realization of the sample $X_1,\dots,X_N$, the estimator may take a different value.

As we mentioned before, empirical risk minimization provides a fundamental method to find estimators, once a suitable loss function has been chosen:
\begin{definition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$, realized by the data set $D =(x_1\dots,x_N)$. Consider the statistical model $\left\{f_\theta : \theta \in \Theta \subseteq \mathbb{R}^d\right\}$. The \textbf{empirical risk minimization (ERM)} method gives the \textbf{ERM estimator} as follows:
\begin{equation}
\hat{\theta}^{ERM}_N(D) := \arg\min_{\theta \in \Theta}\hat{R}_L(f,f_\theta)_N(D).
\end{equation}
\end{definition}

The ERM method is the first optimization problem introduced in these notes, highlighting a fundamental connection between statistical machine learning and optimization theory. If one uses the empirical risk minimization method together with the empirical KL divergence, one obtains the so-called \textbf{maximum likelihood} method:

\begin{definition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$, realized by the data set $D=(x_1\dots,x_N)$. Consider the statistical model $\left\{f_\theta : \theta \in \Theta \subseteq \mathbb{R}^d\right\}$. The ERM method (1.6), along with the \textbf{empirical KL divergence}
\begin{equation}
\hat{R}_L(f,f_\theta)_N(D) := \frac{1}{N}\sum_{k=1}^{N}\ln\frac{f(x_k)}{f_\theta(x_k)} = -\frac{1}{N}\sum_{k=1}^{N}\ln f_\theta(x_k) + \textup{const},
\end{equation}
is named the \textbf{maximum likelihood (ML)} method.
\end{definition}

Combining equations (1.7) and (1.6) yields the following optimization problem, which determines the so-called \textbf{maximum likelihood (ML)} estimator:

\begin{equation}
\hat{\theta}^{ML}_N(D) := \arg\min_{\theta \in \Theta}\left(\sum_{k=1}^N\ln f_\theta(x_k)  \right) = \arg\min_{\theta \in \Theta}\left(\prod_{k=1}^N f_\theta(x_k) \right).
\end{equation}

The two objective functions in (1.8) are referred to as the \textbf{log-likelihood function (LLF)} and the \textbf{likelihood function (LF)}, respectively.
The second equality in (1.8) follows from the monotonicity of the logarithm function, which ensures that the LF and the LLF attain their maximum values at the same point.

The likelihood can be interpreted as the probability of observing the data $D$ given the parameters $\theta$. When $\theta$ is unknown, a natural approach is to estimate it by selecting the values that \textbf{maximize the probability of the observed data}. In the case of i.i.d. data, this probability is given by the likelihood function. This idea underlies the problem stated in (1.8).

Let’s introduce some key concepts related to estimators.
\begin{definition}
The \textbf{mean} (or \textbf{expected value}) of the point estimator $\hat{\theta}_N$ is defined as
\begin{equation}
E_{X_1,\dots,X_N}[\hat{\theta}_N] := \int_{X_1(\Omega)\times \cdots \times X_N(\Omega)} \hat{\theta}_N(x_1,\dots,x_N)f(x_1)\cdots f(x_N) dx_1\cdots dx_N.
\end{equation}
\end{definition}

The joint probability distribution function (PDF) in (1.9) is the product of marginal probability distribution functions because $X_1,\dots,X_N$ are i.i.d..

\begin{definition}
The \textbf{bias} of the point estimator $\hat{\theta}_N$ is defined as
\begin{equation}
BIAS(\hat{\theta}_N,\theta) := E_{X_1,\dots,X_N}[\hat{\theta}_N] - \theta.
\end{equation}
A point estimator is said to be \textbf{unbiased} if its bias is zero. 
\end{definition}

The bias quantifies how far the estimate $\hat{\theta}_N$ is from $\theta$ on average. Heuristically, unbiased estimators are generally more desirable than biased ones because they are, on average, more accurate.

\begin{definition}
The \textbf{variance} of the point estimator $\hat{\theta}_N = (\hat{\theta}_{1},\dots,\hat{\theta}_{d})$ of $\theta \in \Theta \subseteq \mathbb{R}^d$ is defined as
\begin{equation}
\begin{split}
VAR(\hat{\theta}_N) & := E_{X_1,\dots,X_N}[||\hat{\theta}_N-E_{X_1,\dots,X_N}[\hat{\theta}_N]||^2] \\
& = \sum_{k=1}^{d}E_{X_1,\dots,X_N}[(\hat{\theta}_{k}-E_{X_1,\dots,X_N}[\hat{\theta}_{k}])^2] \\
& = \sum_{k=1}^{d}VAR(\hat{\theta}_{k}) = \mathrm{tr}\,COV(\hat{\theta}_N),
\end{split}
\end{equation}
where $\mathrm{tr}\,COV(\hat{\theta}_N)$ denotes the trace of the \textbf{covariance matrix} of $\hat{\theta}_N$.
\end{definition}

\begin{definition}
The \textbf{mean squared error (MSE)} of the point estimator $\hat{\theta}_N$ is defined as
\begin{equation}
MSE(\hat{\theta}_N,\theta) := E_{X_1,\dots,X_N}[||\hat{\theta}_N - \theta||^2].
\end{equation}
\end{definition}

The mean squared error (MSE) is a reasonable criterion for measuring the performance of an estimator. If the MSE is small, we expect that, on average, the resulting estimates are close to the true value. Moreover, the MSE has the interesting property of being decomposable into a sum of bias and variance, as illustrated by the following proposition.

\begin{proposition}
The MSE of an estimator $\hat{\theta}_N$ can be decomposed as a sum of its bias and its variance:
\begin{equation}
MSE(\hat{\theta}_N,\theta) = ||BIAS(\hat{\theta}_N,\theta)||^2 + VAR(\hat{\theta}_N).
\end{equation}
\end{proposition}

This result makes unbiased estimators good candidates due to their zero bias. However, it is important to emphasize that having zero (or small) bias alone is not sufficient to assess an estimator’s quality; the variance in (1.13) also plays a crucial role in evaluating the MSE. A low-bias estimator (i.e., accurate on average) with high variance may produce imprecise predictions and fluctuate significantly depending on the data set $x_1,\dots,x_N$. A good estimator is therefore one that is unbiased and exhibits low variance. 

The MSE is a good criterion for evaluating estimator performance because it quantifies the deviation of the estimator $\hat{\theta}_N$ from the true value $\theta$, as expressed in the following proposition:

\begin{proposition}
Let $\hat{\theta}_N$ be an estimator with finite variance, $VAR(\hat{\theta}_N) < \infty$. Then, for all $\epsilon > 0$
\begin{equation}
P(||\hat{\theta}_N -\theta|| > \epsilon) \leq \frac{MSE(\hat{\theta}_N,\theta)}{\epsilon ^ 2}.
\end{equation}
\end{proposition}

The meaning of this result is clear: if an estimator has a low mean squared error (MSE), then the probability that it produces a value far from the true value is low. Moreover, if we assume that the MSE approaches zero as $N \to \infty$, it follows that the estimator is \textbf{consistent}:

\begin{definition}
The point estimator $\hat{\theta}_N$ is called \textbf{consistent} if its MSE vanishes as $N \to \infty$, that is, it converges in probability to $\theta$:
\begin{equation}
\lim_{N \to \infty} P(||\hat{\theta}_N -\theta|| > \epsilon) = 0.
\end{equation}
\end{definition}

Consistency is a fundamental property that describes the \textbf{asymptotic} behavior of an estimator. It ensures that, as the sample size increases, the estimator produces values that get closer and closer to the parameter $\theta$. Note that the convergence of the mean squared error (MSE) to zero implies consistency; however, the converse is not true in general.

We conclude this section by examining the properties of the empirical risk (1.5) and exploring in which sense it serves as a good approximation of the true risk (1.2). Some general concepts of random variables and estimators will be useful. Let us begin by defining the empirical risk as an estimator of the true risk:

\begin{definition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space $\Omega$. Consider the statistical model $\left\{f_\theta : \theta \in \Theta \subseteq \mathbb{R}^d\right\}$. The empirical risk is an estimator of true risk (1.2) and is defined as the \textbf{empirical mean} of the loss function $L(f(X), f_\theta(X))$, considered as a random variable:
\begin{equation}
\hat{R}_L(f,f_\theta)_N := \overline{L(f(X),f_\theta(X))}_N = \frac{1}{N}\sum_{k=1}^{N}L(f(X_k),f_\theta(X_k)).
\end{equation}
\end{definition}

It is evident that the empirical risk in (1.5) is a realization of the empirical risk estimator in (1.16), and the latter quantity is a random variable depending on the data set $D$.

The empirical risk is useful for efficiently approximating the true risk because it is an \textbf{unbiased} and \textbf{consistent} estimator of the true risk. These properties rely heavily on the assumption that the data set is i.i.d.. Let us now formalize these concepts:

\begin{proposition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space. The empirical risk (1.16) is an \textbf{unbiased} and \textbf{consistent} estimator of true risk (1.2):
\begin{equation}
 E_{X_1,...,X_N}[\hat{R}_L(f,f_\theta)_N] = R_L(f,f_\theta),
\end{equation}
\begin{equation}
 \lim_{N\to \infty}P(|\hat{R}_L(f,f_\theta)_N-R_L(f,f_\theta)| > \epsilon) = 0.
 \end{equation}
\end{proposition}

It is interesting to note that the consistency of the empirical risk is related to the \textbf{law of large numbers}, as stated in the following proposition:
\begin{proposition}
Let $X_1,\dots,X_N \sim f$ be an i.i.d. sample of $\mathbb{R}^n$-valued random variables defined on the same sample space. Consider the loss function $L(f(X), f_\theta(X))$ as a random variable. Therefore, the sequence $L(f(X_1),f_\theta(X_1)),\dots,L(f(X_N),f_\theta(X_N))$ is also i.i.d. and satisfies the \textbf{law of large numbers}. Consequently, the empirical risk estimator defined in (1.16) is consistent.
\end{proposition}

\section{Statistical learning theory foundations} 
This section is dedicated to the \textbf{statistical learning theory} perspective of machine learning. This perspective is explored through the framework of \textbf{probably approximately correct (PAC)} learning. Our aim is to illustrate the principles of this theory and to draw comparisons between the inferential approach and the learning theory approach, highlighting how these two frameworks complement each other.

Learning theory is a complementary paradigm that aims to resolve \textbf{predictive} tasks rather than inferential ones. Roughly speaking, its main goal is not to infer (learn) the distribution that generated the data in $D$ but to use them to learn relations between random variables $X$ and $Y$ (defined on $\Omega$). These relations are expressed as functions (called \textbf{hypotheses}) $h : \mathcal{X} \to \mathcal{Y}$, where $\mathcal{X} \subseteq X(\Omega)$ and $\mathcal{Y} \subseteq Y(\Omega)$.

The learning theory paradigm answers questions beyond those addressed by statistical inference, doing so in a manner that is \textbf{independent of specific statistical models and data distributions}. For example, it investigates how many samples in the data set $D$ are necessary to achieve effective learning, independently of the distribution that generates the data set $D$. The theory states that this sample complexity is closely related to the \textbf{complexity} of the hypothesis—often measured by the number of \textbf{parameters} characterizing the hypothesis—and how it influences both the efficiency of learning and the model’s ability to \textbf{generalize} to unseen data during the learning phase.

These questions are addressed within a probabilistic framework that differs from the inferential one we used, as it essentially leaves \textbf{unspecified} the statistical inference applied to the data set.

Let us formalize the learning problem in this framework:
\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathcal{X}\times \mathcal{Y}$ be an i.i.d. data set. The \textbf{learning problem} aims to learn the parameters $\theta$ that determine the \textbf{hypothesis} $h_\theta : \mathcal{X}\to \mathcal{Y}$ from a \textbf{hypothesis} space $H = \left\{h_\theta : \mathcal{X}\to \mathcal{Y} : \theta \in \Theta \subseteq \mathbb{R}^d\right\}$ such that $h_\theta(x)$ can correctly predict the correct value $y \in \mathcal{Y}$.
\end{definition}

As we mentioned before, the emphasis in learning theory is mainly on prediction through the hypothesis $h_\theta$. Moreover, it is important to stress that the data set $D$ is still generated by a distribution $f(x,y)$.

The Definition 1.14 is quite vague on how it is possible to determine the best hypothesis $h_\theta$ from data set $D$. To formulate a principled method, we use an approach similar to one we used with the formulation of inference in the previous section. Then, we define the \textbf{risk} for learning theory:

 \begin{definition}
Let $(X_1, Y_1),\dots,(X_N, Y_N) \sim f$ be an i.i.d. sample of $(\mathcal{X} \times \mathcal{Y})$-valued random variables defined on the same sample space $\Omega$. Consider the hypothesis space $H = \left\{h_\theta : \mathcal{X} \to \mathcal{Y} :\theta\in\Theta\subseteq\mathbb{R}^d\right\}$. The \textbf{risk} (or \textbf{error}) of the hypothesis $h_\theta$ is defined as
\begin{equation}
R_L(h_\theta) := E_{X,Y}[L(h_\theta(X),Y)] = \int_{\mathcal{X} \times \mathcal{Y}} L(h_\theta(x),y)f(x,y)dxdy,
\end{equation}
where $L :\mathcal{Y} \times \mathcal{Y} \to \mathbb{R}$ is the \textbf{loss} function, and its expression $L(h_\theta(X), Y)$ is a random variable.  
\end{definition}

To obtain the best hypothesis, we need to minimize the risk, to obtain the \textbf{Bayes optimal hypothesis}: 
\begin{equation}
h^* := \arg \min_{h_\theta \in H} R_L(h_\theta). 
\end{equation}

Since the real distribution $f(x,y)$ is generally unknown, one introduces the \textbf{empirical risk} as a proxy for the real risk (recall Definition 1.13):
\begin{definition}
Let $(X_1, Y_1),\dots,(X_N, Y_N) \sim f$ be an i.i.d. sample of $(\mathcal{X} \times \mathcal{Y})$-valued random variables defined on the same sample space $\Omega$. Consider the hypothesis space $\left\{h_\theta : \mathcal{X} \to \mathcal{Y} :\theta\in\Theta\subseteq\mathbb{R}^d\right\}$. The \textbf{empirical risk} is an estimator of true risk (1.19) and is defined as the \textbf{empirical mean} of the loss function $L(h_\theta(X), Y)$, considered as a random variable:
\begin{equation}
\hat{R}_L(h_\theta)_N := \overline{L(h_\theta(X),Y)}_N = \frac{1}{N}\sum_{k=1}^{N}L(h_\theta(X_k),Y_k).
\end{equation}
\end{definition}

It is easy to understand that the empirical risk in learning theory is an \textbf{unbiased} and \textbf{consistent} estimator for the real risk, similar to what we have established in the inference framework.

The empirical risk minimization provides the ERM estimator of learning theory:
\begin{definition}
Let $(X_1, Y_1),\dots,(X_N, Y_N) \sim f$ be an i.i.d. sample of $(\mathcal{X} \times \mathcal{Y})$-valued random variables defined on the same sample space $\Omega$, realized by the data set $D =((x_1, y_1)\dots,(x_N,y_N))$. Consider the hypothesis space $H = \left\{h_\theta : \mathcal{X} \to \mathcal{Y} :\theta\in\Theta\subseteq\mathbb{R}^d\right\}$. The \textbf{empirical risk minimization (ERM)} method gives the \textbf{ERM estimator} as follows:
\begin{equation}
\hat{\theta}^{ERM}_N(D) := \arg\min_{\theta \in \Theta}\hat{R}_L(h_\theta)_N(D).
\end{equation}
\end{definition}

The previous definition illustrates how the value of the parameters corresponding to the minimizer hypothesis can be considered an \textbf{estimator}, because its value varies randomly with the data set $D$. However, in this case, the estimated value does not correspond to any obvious parameter of a probability density function (PDF). In some cases, it is possible to introduce an inferential interpretation for the estimator (1.22) by selecting a suitable loss function $L$, such that the learning theory empirical risk (1.21) coincides with the inferential empirical risk (1.16).

\begin{definition}
The hypothesis space $H = \left\{h : \mathcal{X} \to \mathcal{Y}\right\}$ is \textbf{agnostic PAC learnable} with respect to the loss $L$ if there exists a learning algorithm $A$ and a sample complexity function $N_H : (0,1) ^2 \to \mathbb{R}$ such that for any $\epsilon,\delta \in (0,1)$, for all distributions $f_{X,Y}$ over $\mathcal{X} \times \mathcal{Y}$, and for any i.i.d. data set $D$ that realizes the sequence of random variables $(X_1, Y_1),\dots,(X_N, Y_N)$ drawn from $f_{X,Y}$ with size $N\geq N_H(\epsilon,\delta)$, the following holds:
\begin{equation}
P\left(R_L(A(X_1, Y_1,\dots\,X_N, Y_N)) - \min_{h\in H}R_L(h) > \epsilon\right) < \delta,
\end{equation}
where $A(D)$ is the hypothesis output by algorithm $A$ on data set $D$.
\end{definition}

\chapter{Supervised classification}
In this chapter, we apply the general statistical concepts described in the previous chapter to discuss the problem of \textbf{classification} (also called \textbf{pattern recognition}). The classification problem aims to model the quantitative relationship between an $\mathbb{R}^n$-valued random variable $X$, called \textbf{feature} or \textbf{pattern}, and a random variable $Y$ that takes values in a finite set $C$, called the set of \textbf{classes}. The values of $Y$ are also called \textbf{labels} or \textbf{ground truth}.

Using the i.i.d. data set $D = ((x_1,y_1),\dots,(x_N,y_N))$, the objective is to obtain a \textbf{classification function} (also called a \textbf{classifier} or a \textbf{hypothesis}) $c : \mathbb{R}^n \to C$ that allows us to predict the class $y$ for a new pattern $x$. The inference (learning) process is called \textbf{supervised} since it is "supervised" by utilizing the data set $D$, which is formed by complete pairs of patterns and labels.

\section{Statistical foundation}
At first glance, the classification problem appears simply as a \textbf{prediction} problem that does not seem directly related to any inference problem, as defined in Definition 1.1. In the machine learning literature, often only the predictive aspects are emphasized—that is, the determination of the classification function from data—while the inference principles are left in the background. In contrast, our mathematical discussion begins with the guiding principle illustrated in Definition 1.1, aiming first to provide an inferential definition of classification and then a machine learning (learning theory) definition.

Then, one must find a statistical model and a suitable set of random variables (assumed i.i.d.) for the chosen model. The random variables are clearly $(X_1,Y_1),\dots,(X_N,Y_N)$, while the choice of model is less obvious. To understand how to build a reasonable model, the concept of \textbf{statistical dependence} among random variables can be very helpful:

\begin{definition}
Two random variables $X$ and $Y$, defined on the same sample space $\Omega$, are \textbf{statistically dependent} if for all $x\in X(\Omega)$ and $y \in Y(\Omega)$,
\begin{equation}
f_{X,Y}(x,y) \neq f_X(x)f_Y(y),
\end{equation}
where $f_{X,Y}$ is the \textbf{joint} PDF, and $f_X,f_Y$ are the \textbf{marginal} PDFs of $X$ and $Y$, respectively.
\end{definition}

Using the definition of conditional probability, one can show that (2.1) is equivalent to the following relations:
\begin{equation}
f(y|x) \neq f_Y(y),
\end{equation}
\begin{equation}
f(x|y) \neq f_X(x).
\end{equation}

To perform classification, it is reasonable to assume that the variables $X$ and $Y$ are dependent; otherwise, there would be no correlation between them, rendering the classification problem meaningless. Therefore, we assume dependence between $X$ and $Y$, and consider the conditional probability density function $f(y|x)$, which captures the dependence of $Y$ on $X$, as a suitable model candidate. The following definition formalizes this reasoning:

\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathbb{R}^n\times C$ be an i.i.d. data set, where $C$ is the finite set of classes. The \textbf{classification problem} aims to infer (or learn) the parameters $\theta$ of the statistical model 
\begin{equation}
\left\{f_\theta(y|x) : \theta \in \Theta \subseteq \mathbb{R}^d\right\}
\end{equation}
from the data in $D$. After the learning process, the \textbf{classification function} $c : \mathbb{R}^n \to C$ is calculated as
\begin{equation}
 c_\theta(x) := \arg\max_{y\in C}f_\theta(y|x) = \arg\max_{y\in C}\ln f_\theta(y|x).
\end{equation}
\end{definition}

The classification function has an intuitive interpretation: the class of a pattern $x$ is the most probable value of $y$ conditioned on $x$. Thus, the classification problem, when viewed as a decision problem, is inherently probabilistic in nature.

We provide an equivalent geometric definition of classification:

\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathbb{R}^n\times C$ be an i.i.d. data set, where $C$ is the finite set of classes. The \textbf{classification problem} aims to infer (or learn) from the data in $D$ the parameters $\theta \in \Theta \subseteq \mathbb{R}^d$ of the family of regions of $\mathbb{R}^n$ defined as follows: 
\begin{equation}
\mathcal{R}_C(\theta) := \left\{\mathcal{R}_y(\theta) \subset \mathbb{R}^n : y \in C\right\},
\end{equation}
where the region
\begin{equation}
\mathcal{R}_y(\theta) := \left\{x \in \mathbb{R}^n : f_\theta(y|x) > f_\theta(y'|x), \forall y' \neq y\right\} \subset \mathbb{R}^n
\end{equation}
is named the \textbf{decision rule} for the class $y \in C$. The \textbf{decision boundary} between classes $y$ and $y'$ is defined as:
\begin{equation}
\mathcal{D}_{yy'}(\theta) :=\left\{x \in \mathbb{R}^n : f_\theta(y|x) = f_\theta(y'|x), \forall y' \neq y \right\}.
\end{equation}
Decision rules and decision boundaries satisfy the following relations:
\begin{equation}
\bigcup_{y\in C}\mathcal{R}_y \cup \bigcup_{y'\neq y}\mathcal{D}_{yy'} = \mathbb{R}^n,\,\,\,\mathcal{R}_y\cap\mathcal{R}_{y'} = \emptyset\,\,\,\forall y \neq y'.
\end{equation}
After the learning process, the new pattern $x$ corresponds to class $y$ if $x \in \mathcal{R}_y$.
\end{definition}

The previous definition allows us to observe that the classification problem is equivalent to the geometric problem of finding a family of regions $\mathcal{R}_y(\theta)$, where each region corresponds to the set of patterns $x \in \mathbb{R}^n$ belonging to class $y \in C$. Moreover, these regions, together with their decision boundaries, form a \textbf{partition} of the pattern space (see (2.9)), since any pattern $x$ can belong to only one class $y$.

It is important to emphasize that the decision rules in (2.7) depend on parameters $\theta$, which are random variables learned from data, as discussed in the previous chapter. This implies that the decision rules can be viewed as a type of \textbf{random set}.

\section{Further developments}
The definitions (2.2) and (2.3) are general because we have not specified the explicit form of the statistical model. To make further progress, we need to specify a suitable form of the statistical model (2.4) and perform inference (learning) on its parameters. This specification can be made in two ways, which are termed \textbf{discriminative} and \textbf{generative}. Let us formalize these concepts:

\begin{definition}
A classification problem and its statistical model (2.4), as defined in Definition 2.2, are called \textbf{discriminative} if the goal is to model directly the conditional probability using a suitable parametric function. Otherwise, they are called \textbf{generative} if the goal is to model the conditional probability by applying Bayes' theorem to the joint distribution $f(x,y)$ as follows:
\begin{equation}
f_\theta(y|x) \propto f_\theta(x,y) = f_\theta(y)f_\theta(x|y),
\end{equation}
where the normalization
constant in the denominator is ignored because it is independent of $y$.
\end{definition}

In other words, generative statistical models estimate the joint distribution $f_\theta(x,y)$ and then derive the conditional probability $f_\theta(y|x)$ via Bayes' rule, while discriminative models estimate the conditional probability directly. Generative models are called so because, once the joint distribution is learned, one can \textbf{generate} new data points $(x,y)$ by sampling from this distribution. This simple mathematical principle, implemented in more sophisticated mathematical forms, is the driving force behind so-called \textbf{generative AI}, which generates new data, such as images, text, or audio, by sampling from learned probability distributions.

If the set of classes contains only two elements, the classification is called \textbf{binary}, and we provide the following discriminative definition:

\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathbb{R}^n\times C$ be an i.i.d. data set, where $C = \left\{y_1, y_2 \right\}$ is the finite set of classes. The \textbf{(discriminative) binary classification problem} aims to infer (or learn) from data in $D$ the parameters $\theta$ of the statistical model in (2.4) where the distributions have the \textbf{Bernoullian} form
\begin{equation}
f_\theta(y|x) := \textup{Be}(y;f_\theta(x)) = \left\{\begin{matrix}
f_\theta(x) & y=y_1\\
1 - f_\theta(x) & y = y_2 \\
\end{matrix}\right.,
\end{equation}
where $0 \leq f_\theta(x) \leq 1$ for all $x$ and $\theta$. The \textbf{binary classification function} $c : \mathbb{R}^n \to C$ is calculated as
\begin{equation}
c_\theta(x) := \left\{\begin{matrix}
y_1 & f_\theta(x) > 1/2\\
y_2 & f_\theta(x) < 1/2\\
\end{matrix}\right..
\end{equation}
The \textbf{decision rules} for the classes in $C$ are
\begin{equation}
\mathcal{R}_{y_1}(\theta) :=\left\{ x \in \mathbb{R}^n : f_\theta(y_1|x) > f_\theta(y_2|x) \right\} = \left\{ x \in \mathbb{R}^n : f_\theta(x) > 1/2 \right\},
\end{equation}
\begin{equation}
\mathcal{R}_{y_2}(\theta) := \left\{x \in \mathbb{R}^n : f_\theta(y_1|x) < f_\theta(y_2|x) \right\} = \left\{x \in \mathbb{R}^n : f_\theta(x) < 1/2 \right\}.
\end{equation}
The \textbf{decision boundary} is
\begin{equation}
\mathcal{D}_{y_1y_2}(\theta) :=\left\{x \in \mathbb{R}^n : f_\theta(y_1|x) = f_\theta(y_2|x) = f_\theta(x) = 1/2 \right\}.
\end{equation}
\end{definition}

The generative definition of binary classification can be easily formulated by the reader. For discriminative binary classification problems, a typical choice for the conditional probability model $f_\theta(y_1|x) = f_\theta(x)$ is the \textbf{logistic (sigmoid)} function:
\begin{equation}
\frac{\exp\left ( \theta_0 + \sum_{k=1}^{n}\theta_kx_k \right )}{1+\exp\left ( \theta_0 + \sum_{k=1}^{n}\theta_kx_k \right )}.
\end{equation}

The logistic function is a natural choice for probability modeling because its output is constrained to the interval [0, 1]. This approach to binary classification is known as \textbf{binary logistic classification} (or \textbf{binary logistic regression} in some references) and corresponds to \textbf{half-space} decision rules separated by a \textbf{linear} decision boundary. Data separated by linear decision boundaries are called \textbf{linearly separable}. Let us formalize these results:

\begin{proposition}
Consider the binary logistic classification problem with a Bernoullian distribution defined in (2.11), where
\begin{equation}
f_\theta(y_1|x) := f_\theta(x) = \frac{\exp(b_\theta(x))}{1+\exp(b_\theta(x))},
\end{equation}
\begin{equation}
f_\theta(y_2|x) := 1- f_\theta(x) = \frac{1}{1+\exp(b_\theta(x))},
\end{equation}
\begin{equation}
b_\theta(x) := \theta_0 + \sum_{k=1}^{n}\theta_kx_k.
\end{equation}
The corresponding decision rules are the \textbf{half-spaces}:
\begin{equation}
\mathcal{R}_{y_1}(\theta) = \left\{x \in \mathbb{R}^n : b_\theta(x) > 0 \right\},
\end{equation}
\begin{equation}
\mathcal{R}_{y_2}(\theta) = \left\{ x \in \mathbb{R}^n : b_\theta(x) < 0 \right\}.
\end{equation}
The decision boundary is the hyperplane defined by the Cartesian equation:
\begin{equation}
b_\theta(x) = 0.
\end{equation}
Finally, the classification function is given by:
\begin{equation}
c_\theta(x) = \left\{\begin{matrix}
y_1 & b_\theta(x) > 0\\
y_2 & b_\theta(x) < 0\\
\end{matrix}\right..
\end{equation}
\end{proposition}
If $y_1 = -y_2 = 1$, the classification function can be compactly represented as:
\begin{equation}
c_\theta(x) = \frac{|b_\theta(x)|}{b_\theta(x)}.
\end{equation}

The geometric interpretation of (2.22) and (2.23) is straightforward: a pattern $x$ belongs to class $y_1 (=1)$ if it lies in the "upper" half-space, or to class $y_2(=-1)$ if it lies in the "lower" half-space, relative to the direction of the vector $(\theta_1,\dots,\theta_n) \in \mathbb{R}^n$, which is orthogonal to the separating hyperplane.

The linear binary logistic classification described earlier can be generalized to handle \textbf{nonlinear} decision boundaries by incorporating nonlinear functions $b_\theta(x)$. For example, using quadratic functions in $x$, one can describe decision boundaries represented by \textbf{hyperconics} such as hyperspheres, hyperellipsoids, hyperparaboloids, and so on:
\begin{equation}
 b_\theta(x) := \sum_{k=1}^{n}\sum_{j=1}^{n}q_{kj}(\theta)(x_k-a_k)(x_j - a_j),
\end{equation}
where the coefficients $q_{kj}$ depend on the parameters $\theta$, and $(a_1,\dots,a_n) \in \mathbb{R}^n$ represent the center of the quadratic form.

Let us now consider an important example of the generative classification problem that uses the Gaussian distribution, which is called \textbf{Gaussian discriminant analysis}, and explore its properties.

\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathbb{R}^n\times C$ be an i.i.d. data set, where $C$ is the finite set of classes. The \textbf{Gaussian discriminat analysis} (GDA) is the generative classification problem defined by the following statistical model:
\begin{equation}
f_\theta(y|x) := A \Pi(y)\mathcal{N}(x;\mu(y),\Sigma(y)) = \frac{A\Pi(y)}{\sqrt{2\pi\det \Sigma(y)}}\exp\left [-\frac{1}{2}(x-\mu(y))^T\Sigma^{-1}(y)(x-\mu(y))\right],
\end{equation}
where $A$ is the normalization factor, $\Pi(y)$ is the prior distribution of $y$, and $\mu(y)$ and $\Sigma(y)$ are the mean vector and 
the covariance matrix of the Gaussian associated with class $y$, respectively. Therefore, the parameters of the model are $\theta = (\Pi(y), \mu(y), \Sigma(y))$, for $y \in C$.
\end{definition}

The following proposition illustrates the properties of the decision boundaries in GDA.

\begin{proposition}
For any pair of labels $y$ and $y'\neq y$, the decision boundary of a GDA problem is given by the following Cartesian equation:
\begin{equation}
\frac{1}{2}x^T[\Sigma^{-1}(y)-\Sigma^{-1}(y')]x+[\mu(y')\Sigma^{-1}(y') - \mu(y)\Sigma^{-1}(y)]x + const = 0,
\end{equation}
where $const$ denotes the remaining terms that do not depend on the pattern $x$.
\end{proposition}

The relation (2.27) illustrates that all decision boundaries are \textbf{quadratic}, and for this reason, GDA is also called \textbf{quadratic discriminant analysis}. In the case where all covariance matrices do not depend on class $y$, the decision boundary is \textbf{linear} and GDA is also called \textbf{linear discriminant analysis}.

Another example of a generative classification problem is the so-called \textbf{naive Bayes} problem defined as follows:

\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathbb{R}^n\times C$ be an i.i.d. data set, where $C$ is the finite set of classes. The \textbf{naive Bayes} problem is the generative classification problem defined by the following statistical model:
\begin{equation}
f_\theta(y|x) := A\Pi_\theta(y)f_\theta(x|y) = A \Pi_\theta(y)\prod_{k=1}^nf_\theta(x_k|y),
\end{equation}
where A is the normalization factor, $\Pi_\theta(y)$ is the prior distribution of $y$, and it is assumed that the components of the pattern vector $x = (x_1,\dots,x_n)$ are \textbf{conditionally independent} given the class $y$. This simplifying assumption is known as the \textbf{naive Bayes hypothesis}, and permits the product decomposition in the right-hand side in (2.28).
\end{definition}

\section{Learning theory}
This section is dedicated to the \textbf{learning theory} perspective of binary classification. This perspective is realized through the interaction of two theories: \textbf{probably approximately correct} (PAC) learning and \textbf{Vapnik–Chervonenkis} (VC) theory. Our aim is to illustrate the principles of these theories and, more importantly, to compare the inferential approach with the learning theory approach to understand how the two theories are complementary.

Learning theory is a complementary paradigm that aims to answer questions beyond those addressed by statistical inference, doing so in a way that is \textbf{independent of specific statistical models and data distributions}. For example, it investigates how many samples in the data set $D$ are necessary to achieve effective learning, independently of the distribution that generates the data set $D$. This question is closely related to how the \textbf{complexity} of the model—often measured by the number of parameters in parametric models—affects the efficiency of learning and the model’s ability to \textbf{generalize} to data unseen during the learning phase. In the case of classification, this corresponds to the model’s ability to assign the correct class to an unseen pattern $x$.

These questions are addressed within a probabilistic framework that differs from the inferential one we used, because it essentially leaves unspecified the statistical inference applied to the data set.

Let us formalize the classification problem in this framework:

\begin{definition}
Let $D = ((x_1,y_1),\dots,(x_N,y_N)) \subset \mathbb{R}^n\times C$ be an i.i.d. data set, where $C$ is the finite set of classes. The \textbf{classification problem} aims to learn the parameters $\theta$ that determine the \textbf{hypothesis} $h_\theta : \mathbb{R}^n \to C$ from a \textbf{hypothesis} space $H = \left\{h : \mathbb{R}^n \to C\right\}$ such that $h_\theta$ can correctly predict the class of unseen patterns $x$.
\end{definition}

%For the general problem of classification, the standard maximum likelihood method can be employed to derive estimators for the model parameters:
%\begin{equation}
 %\theta(D) = \arg\min_{\theta \in \Theta} \left [ -\sum_{k=1}^N\ln f_\theta(y_k|x_k)\right].
%\end{equation}

%The target function is referred to as the \textbf{cross-entropy error} because it quantifies the entropy (logarithmic) errors incurred when using the theoretical predictions rather than the empirical ones. Unfortunately, the (4.24) combined with logistic functions (4.13) and (4.14) cannot be optimized exactly, so one must rely on numerical methods.

\chapter{Supervised regression}
In this chapter, we apply the abstract concepts described in the last chapter to discuss the problem of \textbf{regression}. The regression problem aims to model the quantitative relationship between an $\mathbb{R}^m$-valued random variable $Y$, called the \textbf{response} or \textbf{ground truth}, and a $\mathbb{R}^n$-valued random variable $X$, which is called a \textbf{covariate}, \textbf{predictor} or \textbf{feature} in machine learning literature. Regression problems are used to reveal some type of \textbf{correlation} between the (random) quantities $Y$ and $X$, with the goal of \textbf{predicting} the value of response $Y$ from the value of feature $X$.

Using the independent and identically distributed (i.i.d.) data $(x^{(1)},y^{(1)}),\dots,(x^{(N)},y^{(N)})$, the objective is to obtain a \textbf{regression function} (referred to as a \textbf{hypothesis} in machine learning literature) $r : \mathbb{R}^n \to \mathbb{R}^m$ such that $y \approx r(x)$ for unseen data $(x,y)$. If $r$ is a linear function, the problem is called \textbf{linear regression}; otherwise, it is called \textbf{nonlinear regression}. Data $y^{(1)},\dots,y^{(N)}$ are called \textbf{labels}, and the inference (learning) process is called \textbf{supervised} if one uses full sample data $(x,y)$ to "supervise" the learning process.

At first glance, the regression problem appears just as a \textbf{prediction} problem that does not seem correlated to any \textbf{inference} problem, in the sense of Definition 1.1. In the machine learning literature, often, only the predictive aspects are emphasized (the determination of the regression function from data), and the inference principles are left in the background. Instead, our mathematical discussion starts with the guiding principle illustrated in Definition 1.1 with the objective to give first a \textbf{inferential } definition of regression and then a \textbf{machine learning} definition; in the last part of this chapter we will illustrate an alternative formulation of regression that is taken from \textbf{learning theory}. Then, one must
find a statistical model and a set of suitable random variables (assumed i.i.d.) for the chosen model. The random variables are clearly $(X^{(1)},Y^{(1)}),\dots,(X^{(N)},Y^{(N)})$, while the choice of model is less obvious. To understand how to build a reasonable model, the concept of \textbf{statistical dependence} among random variables can be helpful.


\begin{definition}
Let $D = ((x^{(1)},y^{(1)}),\dots,(x^{(N)},y^{(N)})) \subset \mathbb{R}^n \times \mathbb{R}^m$ be an i.i.d. data set. The \textbf{regression problem} aims to infer (or learn) the parameters $\theta$ of the distribution 
\begin{equation}
f_\theta(y|x),
\end{equation}
from data in $D$. The \textbf{regression function} $r : \mathbb{R}^n \to \mathbb{R}^m$ is defined as
\begin{equation}
 r_\theta(x) := E_\theta(Y|X = x) = \int yf_\theta(y|x)dy.
\end{equation}
\end{definition}

In the last definition, the data $x$ is regarded as \textbf{fixed} before inference, this fact is called \textbf{fixed design}. In applications, the final objective is to infer the regression function from data. To explain this related problem, we illustrate an equivalent definition of linear regression that has a more \textbf{machine learning flavor}.

\begin{definition}
Let $D = ((x^{(1)},y^{(1)}),\dots,(x^{(N)},y^{(N)})) \subset \mathbb{R}^n \times \mathbb{R}^m$ be an i.i.d. data set. The \textbf{regression problem} aims to infer (or learn) the parameters $\theta$ from data in $D$, where one assumes the following relation between $X$ and $Y$: 
\begin{equation}
Y = r_\theta(x) + \epsilon,
\end{equation}
where $\epsilon$, called \textbf{noise}, is a $\mathbb{R}^m$-valued random variable with the following properties $(i = 1,\dots,m)$:
\begin{equation}
E(\epsilon_i|X = x) = 0,
\end{equation}
\begin{equation}
VAR(\epsilon_i|X = x) = \sigma^1.
\end{equation}
The function $r$ is named regression function (or hypothesis) and in the case of \textbf{linearity} in parameters $\theta$ $(i = 1,\dots,m)$,
\begin{equation}
r_\theta(x)_i :=  \theta_0^{(i)} + \sum_{j=1}^{n}\theta^{(i)}_jx_j,
\end{equation}
the problem is named \textbf{linear regression problem}. The dimension of parameter space $\Omega \ni \theta$ is $d:= m(n+1)$.
\end{definition}

The random quantity $\epsilon$ represents the fluctuations that cannot be captured by the model, and it is independent of $X$ and $\theta$. These fluctuations represent the intrinsic "error" that one commits in assuming relation (2.6).

The following proposition illustrates how the linear regression problem has a closed solution under certain hypotheses.
\begin{proposition}
Consider the linear regression problem defined in Definition 2.3 with $m=1$, $\epsilon \sim \mathcal{N}(0, \sigma^2)$ and regression function $r_\theta(x)$. Define the following matrices:
\begin{equation}
\bar{y}=\begin{pmatrix}y^{(1)}\\
\vdots\\
y^{(N)}
\end{pmatrix} \in \mathbb{R}^N,
\end{equation}
\begin{equation}
\hat{\theta} = \begin{pmatrix}\hat{\theta}_0\\
\vdots\\
\hat{\theta}_{d-1}\\
\end{pmatrix} \in \mathbb{R}^{d},
\end{equation}
\begin{equation}
\bar{x} = \begin{pmatrix}
1 & x^{(1)}_1 & \cdots & x^{(1)}_{d-1} \\
\vdots & \vdots  & \vdots & \vdots \\
1 & x^{(N)}_1  &\cdots & x^{(N)}_{d-1}
\end{pmatrix} \in \mathbb{R}^{N \times d}.
\end{equation}
Suppose that $\bar{x}^T\bar{x} \in \mathbb{R}^{d \times d}$ is invertible (i.e., $\bar{x}$ has full rank $d \leq N$).
Then, the maximum likelihood method gives the following estimators:
\begin{equation}
\hat{\theta}_N = (\bar{x}^T\bar{x})^{-1}\bar{x}^T\bar{y},
\end{equation}
\begin{equation}
\hat{\sigma}_N^2 = \frac{1}{N}\sum_{k=1}^{N}(y_k-r_{\hat{\theta}_N}(x_k))^2 = \frac{1}{N}||\bar{y} -\bar{x}\hat{\theta}_N||^1.
\end{equation}
\end{proposition}
\textbf{Proof}. Suppose $\epsilon \sim \mathcal{N}(0, \sigma^2)$. Then, the distribution in (2.4) has a \textbf{Gaussian} form with mean $\mu$ depending on $\theta$ and $x$ (because, as stated in (2.5), the mean is the regression function), and variance $\sigma^2$ as another independent parameter that adds up with $\theta$ (recall (1.15)):
\begin{equation}
f_\theta(y|x;\sigma^2) = \mathcal{N}(y;\mu_\theta(x),\sigma^2) = \mathcal{N}(y;r_\theta(x),\sigma^2).
\end{equation}
In other words, one has $Y|X=x \sim \mathcal{N}(r_\theta(x), \sigma^2)$, with the variance in (2.15) the same as $\epsilon$. To estimate the parameters of the regression function, we use the ML method to obtain the LLF (recall (1.12) and (1.13)):
\begin{equation}
-LLF_{D}(\theta,\sigma^2)=-\sum_{k=1}^N\ln\mathcal{N}(y_k;r_\theta(x_k),\sigma^2)=\frac{N}{2}\ln(2\pi\sigma^2)+\frac{1}{2\sigma^2}\sum_{k=1}^N(y_k-r_\theta(x_k))^1.
\end{equation}
We changed sign because we want to convert a maximization problem into a minimization one. Using the MML, the original inference problem was converted into the following function optimization problems:
\begin{equation}
 \theta(D) = \arg\min_{\theta \in \Theta} R_{D}(\theta),
\end{equation}
\begin{equation}
 \sigma^2(D) = \arg\min_{\theta \in \Theta} S_{D}(\sigma^2),
\end{equation}
where the target functions are defined as follows:
\begin{equation}
R_{D}(\theta) =\sum_{k=1}^N(y_k-r_\theta(x_k))^2,
\end{equation}
\begin{equation}
S_{D}(\sigma^2) = \frac{N}{2}\ln(2\pi\sigma^2)+\frac{1}{2\sigma^2}R_{D}(\theta).
\end{equation}
Using definitions in (2.10), (2.11), (2.12), the function in (2.19) can be rewritten using the Euclidean norm as follows:
\begin{equation}
R_{D}(\theta) = ||\bar{y} -\bar{x}\hat{\theta}||^1.
\end{equation}
Calculating the matrix gradient of (2.21) with respect to $\hat{\theta}$ and equating it to zero yields the following matrix equation:
\begin{equation}
\bar{x}^T\bar{x}\hat{\theta} = \bar{x}^T\bar{y}.
\end{equation}
The invertibility of $\bar{x}^T\bar{x}$ yields (2.13), while the optimization problem (2.18), (2.20) with (2.13) has the solution in (2.14).\\
$\square$\\
The target function (2.19) is defined as a sum of squared differences between the empirical prediction $y_k$ and the theoretical prediction $r_\theta(x_k)$. This is referred to as the \textbf{squared error} because it quantifies the squared errors incurred when using the theoretical predictions rather than the empirical ones. Additionally, the problem defined in (2.17) and (2.19) is known as the \textbf{least squares problem}, and the resulting estimator in (2.13) is called the \textbf{ordinary least squared (OLS) estimator}. The equation (2.13) defines the unique minimizer because the target function (2.21) is strictly \textbf{convex}. Indeed, the Hessian of (2.21) is equal to $\bar{x}^T\bar{x}$ that is a full-rank, symmetric and positive-definite matrix under the hypotheses of Proposition 2.1.

Multiplying (2.13) by $\bar{x}$, one obtains that
\begin{equation}
\bar{x}\hat{\theta} = \bar{x}(\bar{x}^T\bar{x})^{-1}\bar{x}^T\bar{y}.
\end{equation}
The geometric interpretation of (2.23) is significant, as the vector $\bar{x}\hat{\theta}$ is the \textbf{orthogonal projection} of $N$-dimensional vector $\bar{y}$ in the $d$-dimensional column space of $\bar{x}$. The reader can prove that the matrix
\begin{equation}
P = \bar{x}(\bar{x}^T\bar{x})^{-1}\bar{x}^T
\end{equation}
is a projection, i.e., $P^2 = P$.

The following proposition illustrates some properties of estimators (2.13).

\begin{proposition}
The estimator in (2.13) has the following properties:
\begin{itemize}
\item The estimator is unbiased and distributed according to $\mathcal{N}(\theta, COV(\hat{\theta}_N))$, where 
\begin{equation}
COV(\hat{\theta}_N) = \hat{\sigma}_N^2(\bar{x}^T\bar{x})^{-1}.
\end{equation}
\item The estimator has variance
\begin{equation}
VAR(\hat{\theta}_N) = \hat{\sigma}^2_N\textup{tr}((\bar{x}^T\bar{x})^{-1}) \sim d/N.
\end{equation}
\item The estimator is consistent: $MSE(\hat{\theta}_N) = VAR(\hat{\theta}_N) \to 0$ if $N \to \infty$. 
\end{itemize}
\end{proposition}

What we have illustrated in Proposition 2.1 is the first machine learning procedure of these notes. The outputs of the procedure are the estimators in (2.13) and (2.14). But more important for applications is the estimator (2.13) that contributes to the definition of the regression function through (2.6).

We now focus on some properties of the regression function $r_\theta(x)$ and illustrate an important relation which is very similar to the decomposition that we illustrated in Proposition 1.1. The main idea is noting that for any fixed value $x$ the quantity $r_{\hat{\theta}_N}(x)$ is a random variable that \textbf{estimates} the real value of the regression function, that we denote as $r(x)$. So, we can expect that there exists a similar relation to (1.8) that is applied to the regression function.

\begin{proposition}
Let $D = ((x^{(1)},y^{(1)}),\dots,(x^{(N)},y^{(N)}))$ be an i.i.d. data set and consider the regression problem of Definition 2.2. For any fixed value $x$ the quantity $r_{\hat{\theta}_N}(x)$ is an estimator of the true value of the regression function $r(x) = E(Y|X = x)$, corresponding to the estimator $\hat{\theta}_N$. The MSE of the estimator $r_{\hat{\theta}_N}(x)$ is given by:
\begin{equation}
\begin{split}
MSE(r_{\hat{\theta}_N}(x),r(x)) &= E_D(||r_{\hat{\theta}_N}(x) - r(x)||^2) \\ 
&=||E_D(r_{\hat{\theta}_N}(x)) - r(x)||^2+E_D(||E_D(r_{\hat{\theta}_N}(x)) - r_{\hat{\theta}_N}(x)||^2)\\
&=||BIAS(r_{\hat{\theta}_N}(x), r(x))||^2 + VAR(r_{\hat{\theta}_N}(x)),
\end{split}
\end{equation}
where $E_D = E_{X_1,Y_1,\dots,X_N,Y_N}$.
\end{proposition}
It is easy to see that the variance in (2.27) is proportional to the variance of $\hat{\theta}_N$ in Proposition 2.1. Then, if the variance of $\hat{\theta}_N$ is high, we expect a higher variance in (2.27) and lower performance of the regression function $r_{\hat{\theta}_N}(x)$ when predicting \textbf{unseen} data $(x,y)$, i.e., data not belonging to the training set. Recall that the estimators are random variables, and in the case of high variance, it is probable that they exhibit significant variability across different data sets.

Let us explain this concept in more depth, as it is crucial for the practical application of (linear) regression. Consider two data sets $D = ((x^{(1)},y^{(1)}),\dots,(x^{(N)},y^{(N)}))$, and $D' = ((x'^{(1)},y'^{(1)}),\dots,(x'^{(N)},y'^{(N)}))$. The first data set $D$ is used to resolve the regression problem and obtain the estimator $\hat{\theta}_N$ using formula (2.13). We could think to assess the quality of the estimator $\hat{\theta}_N$ by comparing the prediction of the regression function $r_{\hat{\theta}_N}(x'^{(i)})$ with the actual value $y'^{(i)}$ for $i = 1,\dots,N$; this procedure is referred to as \textbf{model validation} in the literature, and it is a popular method for verifying the performance of fitted models. If the variance of $r_{\hat{\theta}_N}(x)$ is high, we expect that the validation will reveal significant discrepancies between the predictions and the actual values of $D'$. This occurs because it is "unlikely" that the correct value of the parameters $\theta$ describing data $D'$ is still $\hat{\theta}_N$ which were obtained from $D$; therefore, the predictions are not optimal for data outside of the training set.

When the trained model can accurately describe the training data but predicts poorly for unseen data, the model is said to be \textbf{overfitted}, and this case corresponds to low bias and high variance. On the other side, when the trained model can accurately describe both the training data and unseen data, the model is said to have good \textbf{generalization} power. Another possible situation is that the trained model cannot accurately describe the training data and possibly can describe correctly unseen data (this is not strange as it can appear): the model is said to be \textbf{underfitted}, and this case corresponds to a high bias and low variance of the estimator. In the case of linear regression, one can have underfitting if the linearity of the regression function is not enough to capture the relations between observed data; this implies that the hypotheses of Definition 2.3 and the conclusions of Proposition 2.2 are not valid anymore. Indeed, these last two results are valid if one can assume that the linear form is suitable \textbf{at least} for data in the data set $D$.

The Proposition 2.2 facilitates a deeper understanding of how overfitting manifests in the context of linear regression. Specifically, it illustrates how the variance of the estimator $\hat{\theta}_N$ in (2.13) depends on the ratio $d/N$, where $d$ is the dimension of the parameter space and $N$ is the size of the data set. Then, it is clear that if $d$ is equal to data set size $N$, the variance reaches its maximum (recall that $d \leq N$ according to Proposition 2.1). Indeed, in this case, the optimization problem in (2.21) becomes an ordinary linear system problem that can be solved through algebraic methods if the matrix $\bar{x}$ is invertible, giving the exact solution $\hat{\theta}_N = \bar{x}^{-1}y$ and a vanishing value for target, $R(\hat{\theta}_N) = 0$; therefore, the process of training \textbf{memorizes} exactly the data, and this leads to severe overfitting and unoptimal generalization on unseen data. Then, the practical recipe to have an effective linear regression function is to have more data than parameters, in order to reduce the ratio $d/N$. This is clearly intuitive: the more data that are available, the more the probability of learning a complete view of the data generation process.

In the general case when $d > N$, the matrix $\bar{x}^T\bar{x}$ is not invertible and the calculation of the estimator must be done numerically from (2.22). Non-invertibility implies that row vectors in (2.12) are \textbf{linearly dependent (or collinear)}. The thumb rule about $d/N$ can be applied also to this case and suggests that overfitting is related to collinearity of feature vectors.

In a more general context, the rule of thumb derived from the ratio $d/N$ is generalized by introducing the concept of \textbf{complexity} (or \textbf{expressiveness}) of the regression function (or hypothesis) $r$. If the complexity of $r$ is too high (e.g. the function $r$ is not linear but rather a high-degree polynomial or it is a highly non-linear function, as one encounters in \textbf{deep learning} with \textbf{neural networks}), the phenomenon of overfitting can occur when the data number $N$ is much less than this complexity. Unfortunately, in these more complex contexts, the complexity of a model is not directly related to the number of parameters and it is quite challenging to estimate the complexity of more intricate inference problems, such as those associated with deep learning. The complexity can also depend on \textbf{algorithms} used to obtain estimators.

A general principle that characterizes the behavior of bias and variance of estimators is called the \textbf{bias-variance trade-off} and it is quantitatively expressed in Proposition 2.3: according to this principle, an overfitted model corresponds to low-bias and high-variance estimators, while an underfitted model corresponds to high-bias and low-variance estimators. Effective models must be able to handle these two opposite effects in order to reduce the MSE of estimators.

To reduce the phenomenon of overfitting in regression, in the literature were introduced modified versions of regression that are \textbf{regularized}. Let us define this more precisely.

\begin{definition}
Let $D = ((x^{(1)},y^{(1)}),\dots,(x^{(N)},y^{(N)}))$ be an i.i.d. data set. The regression problem defined in Definition 2.3 is called \textbf{regularized (or $L^2-$regularized, or ridge)} if one assumes the following form for the probability distribution in (2.4): 
\begin{equation}
f_\theta(y|x;\sigma^2) := \alpha\mathcal{N}(y;r_\theta(x),\sigma^2)f(\theta, \sigma^2),
\end{equation}
where $\alpha$ is an unimportant normalization constant and $f$ is defined as follows:
\begin{equation}
f(\theta,\sigma^2) := \mathcal{N}(\theta;0,\lambda^{-1}I_{d\times d}) = \frac{1}{\sqrt{2\pi\lambda^{-d}}}\exp(-||\theta||^2\lambda / 2).
\end{equation}
In (2.29) $\lambda > 0$ is a constant termed \textbf{regularization hyperparameter}, and $I_{d\times d}$ is the $d\times d$ identity matrix, where $d$ is the  dimension of parameter space $\Theta \ni \theta$.
\end{definition}

The interpretation of (2.28) may seem obscure within the frequentist approach to inference that we have adopted, but it is clear in \textbf{Bayesian} approach where (2.28) serves as the \textbf{prior} of $\theta$ and $\sigma^2$. The regularized linear regression problem, as defined, has a closed-form solution, which is illustrated by the following proposition.

\begin{proposition}
Consider the regularized regression problem defined in Definition 2.4 with $m=1$, and with a linear regression function as defined in (2.6) and (2.9). Define the matrix quantities as in (2.10), (2.11), and (2.12). Then, the maximum likelihood method gives the following estimator for $\theta \in \Theta \subseteq \mathbb{R}^d$:
\begin{equation}
\hat{\theta}_N^{(\lambda)} = (\bar{x}^T\bar{x} +N\lambda I_{d\times d})^{-1}\bar{x}^T\bar{y},
\end{equation}
while the estimator for $\hat{\sigma}_N^{(\lambda)2}$ is the same as (2.14).
\end{proposition}
\textbf{Proof}. Assume (2.28) and (2.29). To estimate the parameters of the regression function, we use the usual ML method to obtain the LLF (recall (1.12) and (1.13)):
\begin{equation}
\begin{split}
-LLF_{D}(\theta,\sigma^2)&=-\sum_{k=1}^N\ln\mathcal{N}(y_k;r_\theta(x_k),\sigma^2) - N\ln f(\theta,\sigma^2)\\
&=\frac{N}{2}\ln(2\pi\sigma^2)+\frac{1}{2\sigma^2}\sum_{k=1}^N(y_k-r_\theta(x_k))^2 -\frac{N}{2}\ln\left (\frac{\lambda^d}{2\pi}\right)+\frac{N\lambda}{2}||\theta||^1.
\end{split}
\end{equation}
We changed sign as usual and removed the constant $\alpha$ because it is unimportant. Using the MML, the original inference problem was converted into the following function optimization problems
\begin{equation}
 \theta(D) = \arg\min_{\theta \in \Theta} R_{D}(\theta),
\end{equation}
\begin{equation}
 \sigma^2(D) = \arg\min_{\theta \in \Theta} S_{D}(\sigma^2),
\end{equation}
where the target functions are defined as follows:
\begin{equation}
R_{D}(\theta) =\sum_{k=1}^N(y_k-r_\theta(x_k))^2 + \frac{N\lambda}{2}||\theta||^2,
\end{equation}
\begin{equation}
S_{D}(\sigma^2) = \frac{N}{2}\ln(2\pi\sigma^2)+\frac{1}{2\sigma^2}(R_{D}(\theta)-N\lambda||\theta||^2/2).
\end{equation}
Using definitions in (2.10), (2.11), (2.12), the function in (2.34) can be rewritten using the Euclidean norm as follows:
\begin{equation}
R_{D}(\theta) = ||\bar{y} -\bar{x}\hat{\theta}||^2 + N\lambda||\hat{\theta}||^2 / 1.
\end{equation}
Calculating the matrix gradient of (2.36) with respect to $\hat{\theta}$ and equating it to zero yields the following matrix equation (we absorbed 1/2 in $\lambda)$:
\begin{equation}
(\bar{x}^T\bar{x}+ N\lambda I_{d\times d})\hat{\theta} = \bar{x}^T\bar{y}.
\end{equation}
The regularization hyperparameter makes the left-hand side matrix in (2.37) always invertible, then one obtains (2.30), while the optimization problem (2.34), (2.35) with (2.30) has the solution in (2.14).\\
$\square$\\
The resulting estimator in (2.30) is called the \textbf{regularized least squared (RLS) estimator} and it is the unique minimizer because the target function (2.34) is strictly \textbf{convex}. The following proposition illustrates some properties of the regularized estimator (2.30).

\begin{proposition}
The estimator in (2.30) has the following properties for all $\lambda \geq 0$:
\begin{itemize}
\item The estimator is distributed according to $\mathcal{N}(\hat{\theta}_N^{(\lambda)}, COV(\hat{\theta}_N^{(\lambda)}))$, where 
\begin{equation}
E(\hat{\theta}_N^{(\lambda)}) =  (\bar{x}^T\bar{x}+N\lambda I_{d\times d})^{-1}\bar{x}^T\bar{x}\theta = \theta -N\lambda (\bar{x}^T\bar{x}+N\lambda I_{d\times d})^{-1}\theta,
\end{equation}
\begin{equation}
COV(\hat{\theta}_N^{(\lambda)}) =\hat{\sigma}_N^{(\lambda)2} (\bar{x}^T\bar{x}+N\lambda I_{d\times d})^{-1}\bar{x}^T\bar{x}(\bar{x}^T\bar{x}+N\lambda I_{d\times d})^{-1}.
\end{equation}
\item The estimator is biased with
\begin{equation}
BIAS(\hat{\theta}_N^{(\lambda)}) = -N\lambda (\bar{x}^T\bar{x}+N\lambda I_{d\times d})^{-1}\theta \geq BIAS(\hat{\theta}_N) = 0.
\end{equation}
\item The estimator has variance:
\begin{equation}
VAR(\hat{\theta}_N^{(\lambda)}) = \textup{tr}(COV(\hat{\theta}_N^{(\lambda)})) \sim \frac{d}{N(\lambda + 1)^2} \leq VAR(\hat{\theta}_N).
\end{equation}
\item The estimator has a \textbf{shrinking} Euclidean norm when $\lambda \to \infty$:
\begin{equation}
||\hat{\theta}_N^{(\lambda)}|| \sim \lambda^{-1}.
\end{equation}
\item The MSE of the estimator has the following qualitative behavior:
\begin{equation}
MSE(\hat{\theta}_N^{(\lambda)}) \sim \lambda^4 +  \frac{d}{N(\lambda + 1)^2}.
\end{equation}
\end{itemize}
\end{proposition}

The relation (2.42) is related to the form of (2.34). The term proportional to $\lambda$ acts as a \textbf{penalty} term that penalizes estimators with high values of the Euclidean norm. In the literature, this property is called \textbf{shrinkage}. While the relations (2.40) and (2.41) illustrate that the bias and the variance are always \textbf{not less} and \textbf{not greater} that the equivalent for OLS estimator. Moreover, the relation (2.43) is a lucid realization of the \textbf{bias-variance trade-off} illustrated before: if $\lambda$ is too high, the bias is high (low performance on training data) and the variance is low (good performance on unseen data); otherwise, the bias is low (good performance on training data) and the variance is high (low performance on unseen data).
\end{document}
